{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "using Statistics  # mean()\n",
    "using Random  # randperm()\n",
    "using Dates\n",
    "\n",
    "include(\"./Functional.jl\")\n",
    "include(\"./Layer.jl\")\n",
    "include(\"./Optimizer.jl\")\n",
    "include(\"./MNIST.jl\")\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Random.seed!(2019)\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(size(x_train), size(x_test), size(y_train), size(y_test)) = ((784, 60000), (784, 10000), (10, 60000), (10, 10000))\n"
     ]
    }
   ],
   "source": [
    "_x_train = MNIST.images(:train)\n",
    "_x_test = MNIST.images(:test)\n",
    "_y_train = MNIST.labels(:train)\n",
    "_y_test = MNIST.labels(:test)\n",
    "\n",
    "x_train = convert(Array{Float64, 2}, hcat([vec(Float64.(x)) for x in _x_train]...))\n",
    "x_test = convert(Array{Float64, 2}, hcat([vec(Float64.(x)) for x in _x_test]...))\n",
    "y_train = Functional.onehot(Float64, _y_train, 0:9)\n",
    "y_test = Functional.onehot(Float64, _y_test, 0:9)\n",
    "@show size(x_train), size(x_test), size(y_train), size(y_test)\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function dataloader(x, y, ;batch_size=1, shuffle=false)\n",
    "    function producer(c::Channel, x, y, batch_size, shuffle)\n",
    "        data_size = size(x, 2)\n",
    "        if shuffle\n",
    "            randidx = randperm(data_size)\n",
    "            x = x[:, randidx]\n",
    "            y = y[:, randidx]\n",
    "        end\n",
    "        i = 1\n",
    "        while i < data_size-batch_size\n",
    "            put!(c, (x[:, i:i+batch_size-1], y[:, i:i+batch_size-1]))\n",
    "            i += batch_size\n",
    "        end\n",
    "        put!(c, (x[:, i:end], y[:, i:end]))\n",
    "    end\n",
    "\n",
    "    ch = Channel((ch_arg) -> producer(ch_arg, x, y, batch_size,  shuffle))\n",
    "    return ch\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mutable struct TwoLayerNet{T}\n",
    "    a1lyr::Layer.AffineLayer{T}\n",
    "    relu1lyr::Layer.ReluLayer\n",
    "    bn::Layer.BatchNormalization\n",
    "    a2lyr::Layer.AffineLayer{T}\n",
    "    softmaxlyr::Layer.SoftmaxWithLossLayer{T}\n",
    "    params\n",
    "end\n",
    "\n",
    "function (::Type{TwoLayerNet{T}})(input_size::Int, hidden_size::Int, output_size::Int; weight_init_std::Float64=0.01) where T\n",
    "    W1 = weight_init_std .* randn(T, hidden_size, input_size)\n",
    "    b1 = zeros(T, hidden_size)\n",
    "    W2 = weight_init_std .* randn(T, output_size, hidden_size)\n",
    "    b2 = zeros(T, output_size)\n",
    "    gamma = ones(hidden_size)\n",
    "    beta = zeros(hidden_size)\n",
    "    a1lyr = Layer.AffineLayer(W1, b1)\n",
    "    relu1lyr = Layer.ReluLayer()\n",
    "    bn = Layer.BatchNormalization(gamma, beta)\n",
    "    a2lyr = Layer.AffineLayer(W2, b2)\n",
    "    softmaxlyr = Layer.SoftmaxWithLossLayer{T}()\n",
    "    params = [a1lyr.W, a1lyr.b, a2lyr.W, a2lyr.b, bn.gamma, bn.beta]\n",
    "    TwoLayerNet(a1lyr, relu1lyr, bn, a2lyr, softmaxlyr, params)\n",
    "end\n",
    "\n",
    "function setparams(net::TwoLayerNet, params)\n",
    "    net.a1lyr.W = params[1]\n",
    "    net.a1lyr.b = params[2]\n",
    "    net.a2lyr.W = params[3]\n",
    "    net.a2lyr.b = params[4]\n",
    "    net.bn.gamma = params[5]\n",
    "    net.bn.beta = params[6]\n",
    "end\n",
    "\n",
    "function predict(net::TwoLayerNet{T}, x::AbstractArray{T}) where T\n",
    "    a1 = Layer.forward(net.a1lyr, x)\n",
    "    z1 = Layer.forward(net.relu1lyr, a1)\n",
    "    z2 = Layer.forward(net.bn, z1)\n",
    "    a2 = Layer.forward(net.a2lyr, z2)\n",
    "    return a2\n",
    "end\n",
    "\n",
    "function criterion(net::TwoLayerNet{T}, x::AbstractArray{T}, t::AbstractArray{T}) where T\n",
    "    y = predict(net, x)\n",
    "    Layer.forward(net.softmaxlyr, y, t)\n",
    "end\n",
    "\n",
    "function accuracy(net::TwoLayerNet{T}, x::AbstractArray{T}, t::AbstractArray{T}) where T\n",
    "    y = vec(mapslices(argmax, predict(net, x), dims=1))\n",
    "    t1 = vec(mapslices(argmax, t, dims=1))\n",
    "    mean(y .== t1)\n",
    "end\n",
    "\n",
    "function gradient(net::TwoLayerNet{T}, x::AbstractArray{T}, t::AbstractArray{T}) where T\n",
    "    # forward\n",
    "    criterion(net, x, t)\n",
    "    # backward\n",
    "    dout = one(T)\n",
    "    dz2 = Layer.backward(net.softmaxlyr, dout)\n",
    "    da2 = Layer.backward(net.a2lyr, dz2)\n",
    "    db2 = Layer.backward(net.bn, da2)\n",
    "    dz1 = Layer.backward(net.relu1lyr, db2)\n",
    "    da1 = Layer.backward(net.a1lyr, dz1)\n",
    "    [net.a1lyr.dW, net.a1lyr.db, net.a2lyr.dW, net.a2lyr.db, net.bn.dgamma, net.bn.dbeta]\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "const iters_num = 10000\n",
    "const batch_size = 100\n",
    "const learning_rate = Float64(1e-2)\n",
    "const train_size = size(x_train, 2) # => 60000\n",
    "const iter_per_epoch = max(train_size / batch_size, 1)  # => 600\n",
    "\n",
    "network = TwoLayerNet{Float64}(784, 50, 10)\n",
    "optimizer = Optimizer.Adam(network, learning_rate)\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0-element Array{Float64,1}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss_list = Float64[]\n",
    "train_acc_list = Float64[]\n",
    "test_acc_list = Float64[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 train_acc=0.6551 / test_acc=0.656\n",
      "2 train_acc=0.7339333333333333 / test_acc=0.7327\n",
      "3 train_acc=0.7871 / test_acc=0.7939\n",
      "4 train_acc=0.8319666666666666 / test_acc=0.8402\n",
      "5 train_acc=0.8197166666666666 / test_acc=0.8232\n",
      "6 train_acc=0.8518 / test_acc=0.8518\n",
      "7 train_acc=0.8472833333333334 / test_acc=0.8548\n",
      "8 train_acc=0.85035 / test_acc=0.8552\n",
      "9 train_acc=0.8565333333333334 / test_acc=0.8586\n",
      "10 train_acc=0.8698666666666667 / test_acc=0.877\n",
      "11 train_acc=0.8723666666666666 / test_acc=0.8786\n",
      "12 train_acc=0.8728666666666667 / test_acc=0.875\n",
      "13 train_acc=0.8847666666666667 / test_acc=0.8909\n",
      "14 train_acc=0.8744166666666666 / test_acc=0.8736\n",
      "15 train_acc=0.8837666666666667 / test_acc=0.8903\n",
      "16 train_acc=0.8967166666666667 / test_acc=0.9005\n",
      "17 train_acc=0.9003833333333333 / test_acc=0.902\n",
      "18 train_acc=0.90105 / test_acc=0.9046\n",
      "19 train_acc=0.90205 / test_acc=0.9014\n",
      "20 train_acc=0.9017666666666667 / test_acc=0.8998\n",
      "21 train_acc=0.9064333333333333 / test_acc=0.9091\n",
      "22 train_acc=0.9111666666666667 / test_acc=0.9119\n",
      "23 train_acc=0.9037666666666667 / test_acc=0.9014\n",
      "24 train_acc=0.9018833333333334 / test_acc=0.9072\n",
      "25 train_acc=0.9088166666666667 / test_acc=0.9109\n",
      "26 train_acc=0.9158333333333334 / test_acc=0.9133\n",
      "27 train_acc=0.9152666666666667 / test_acc=0.9145\n",
      "28 train_acc=0.9171166666666667 / test_acc=0.9184\n",
      "29 train_acc=0.9212333333333333 / test_acc=0.9197\n",
      "30 train_acc=0.9189 / test_acc=0.9196\n",
      "31 train_acc=0.92185 / test_acc=0.9223\n",
      "32 train_acc=0.9219833333333334 / test_acc=0.9236\n",
      "33 train_acc=0.92445 / test_acc=0.9232\n",
      "34 train_acc=0.9245333333333333 / test_acc=0.9253\n",
      "35 train_acc=0.9264833333333333 / test_acc=0.9266\n",
      "36 train_acc=0.9237166666666666 / test_acc=0.922\n",
      "37 train_acc=0.9295833333333333 / test_acc=0.926\n",
      "38 train_acc=0.9276666666666666 / test_acc=0.9304\n",
      "39 train_acc=0.9299 / test_acc=0.9294\n",
      "40 train_acc=0.9309333333333333 / test_acc=0.9342\n",
      "41 train_acc=0.9309833333333334 / test_acc=0.9309\n",
      "42 train_acc=0.9338333333333333 / test_acc=0.9327\n",
      "43 train_acc=0.9351 / test_acc=0.9337\n",
      "44 train_acc=0.9365 / test_acc=0.9353\n",
      "45 train_acc=0.9362333333333334 / test_acc=0.9323\n",
      "46 train_acc=0.93475 / test_acc=0.9332\n",
      "47 train_acc=0.9369166666666666 / test_acc=0.9359\n",
      "48 train_acc=0.9353166666666667 / test_acc=0.9341\n",
      "49 train_acc=0.9373166666666667 / test_acc=0.9331\n",
      "50 train_acc=0.93825 / test_acc=0.9359\n",
      "51 train_acc=0.94165 / test_acc=0.9404\n",
      "52 train_acc=0.9412166666666667 / test_acc=0.937\n",
      "53 train_acc=0.9396 / test_acc=0.9374\n",
      "54 train_acc=0.9408833333333333 / test_acc=0.9365\n",
      "55 train_acc=0.9395 / test_acc=0.9354\n",
      "56 train_acc=0.9434333333333333 / test_acc=0.9389\n",
      "57 train_acc=0.9439166666666666 / test_acc=0.9428\n",
      "58 train_acc=0.9468666666666666 / test_acc=0.9454\n",
      "59 train_acc=0.9470833333333334 / test_acc=0.9459\n",
      "60 train_acc=0.9473333333333334 / test_acc=0.9446\n",
      "61 train_acc=0.9446166666666667 / test_acc=0.9429\n",
      "62 train_acc=0.94185 / test_acc=0.9394\n",
      "63 train_acc=0.9480666666666666 / test_acc=0.9469\n",
      "64 train_acc=0.9494833333333333 / test_acc=0.9481\n",
      "65 train_acc=0.9490833333333333 / test_acc=0.9484\n",
      "66 train_acc=0.94965 / test_acc=0.947\n",
      "67 train_acc=0.9444666666666667 / test_acc=0.9421\n",
      "68 train_acc=0.9519333333333333 / test_acc=0.9505\n",
      "69 train_acc=0.95155 / test_acc=0.9502\n",
      "70 train_acc=0.9514166666666667 / test_acc=0.9499\n",
      "71 train_acc=0.95265 / test_acc=0.9525\n",
      "72 train_acc=0.9526 / test_acc=0.9493\n",
      "73 train_acc=0.9518166666666666 / test_acc=0.9497\n",
      "74 train_acc=0.9542333333333334 / test_acc=0.9504\n",
      "75 train_acc=0.9541666666666667 / test_acc=0.9519\n",
      "76 train_acc=0.95465 / test_acc=0.9551\n",
      "77 train_acc=0.9530333333333333 / test_acc=0.9502\n",
      "78 train_acc=0.9552666666666667 / test_acc=0.9532\n",
      "79 train_acc=0.9511 / test_acc=0.951\n",
      "80 train_acc=0.9548333333333333 / test_acc=0.9502\n",
      "81 train_acc=0.9549666666666666 / test_acc=0.9529\n",
      "82 train_acc=0.9579166666666666 / test_acc=0.9536\n",
      "83 train_acc=0.9572666666666667 / test_acc=0.9543\n",
      "84 train_acc=0.9549166666666666 / test_acc=0.9531\n",
      "85 train_acc=0.9565333333333333 / test_acc=0.9515\n",
      "86 train_acc=0.9581666666666667 / test_acc=0.9556\n",
      "87 train_acc=0.9580666666666666 / test_acc=0.9536\n",
      "88 train_acc=0.9583166666666667 / test_acc=0.9545\n",
      "89 train_acc=0.9595166666666667 / test_acc=0.9568\n",
      "90 train_acc=0.9583666666666667 / test_acc=0.9561\n",
      "91 train_acc=0.9587333333333333 / test_acc=0.9574\n",
      "92 train_acc=0.9608666666666666 / test_acc=0.9591\n",
      "93 train_acc=0.9609833333333333 / test_acc=0.9605\n",
      "94 train_acc=0.9614833333333334 / test_acc=0.9601\n",
      "95 train_acc=0.9618166666666667 / test_acc=0.9588\n",
      "96 train_acc=0.95995 / test_acc=0.9563\n",
      "97 train_acc=0.9619833333333333 / test_acc=0.9599\n",
      "98 train_acc=0.9612333333333334 / test_acc=0.9589\n",
      "99 train_acc=0.9625 / test_acc=0.959\n",
      "100 train_acc=0.9593833333333334 / test_acc=0.9555\n",
      "loss = 26.406073322564698\n",
      "0: train_acc=0.9593833333333334 / test_acc=0.9555\n",
      "117.582\n",
      "1 train_acc=0.9620833333333333 / test_acc=0.9601\n",
      "2 train_acc=0.9620166666666666 / test_acc=0.9591\n",
      "3 train_acc=0.9628833333333333 / test_acc=0.9596\n",
      "4 train_acc=0.9639833333333333 / test_acc=0.9616\n",
      "5 train_acc=0.9648833333333333 / test_acc=0.9604\n",
      "6 train_acc=0.9628 / test_acc=0.9594\n",
      "7 train_acc=0.9619333333333333 / test_acc=0.9596\n",
      "8 train_acc=0.9618 / test_acc=0.9582\n",
      "9 train_acc=0.9594166666666667 / test_acc=0.9566\n",
      "10 train_acc=0.9607166666666667 / test_acc=0.9584\n",
      "11 train_acc=0.9628 / test_acc=0.9603\n",
      "12 train_acc=0.96395 / test_acc=0.9622\n",
      "13 train_acc=0.9633666666666667 / test_acc=0.962\n",
      "14 train_acc=0.9643666666666667 / test_acc=0.9623\n",
      "15 train_acc=0.9622833333333334 / test_acc=0.9582\n",
      "16 train_acc=0.9641666666666666 / test_acc=0.96\n",
      "17 train_acc=0.9638333333333333 / test_acc=0.9584\n",
      "18 train_acc=0.9622 / test_acc=0.9575\n",
      "19 train_acc=0.9635166666666667 / test_acc=0.9584\n",
      "20 train_acc=0.9639833333333333 / test_acc=0.9602\n",
      "21 train_acc=0.96195 / test_acc=0.9584\n",
      "22 train_acc=0.9639166666666666 / test_acc=0.961\n",
      "23 train_acc=0.9644 / test_acc=0.9598\n",
      "24 train_acc=0.9628 / test_acc=0.9585\n",
      "25 train_acc=0.9634166666666667 / test_acc=0.9585\n",
      "26 train_acc=0.9627333333333333 / test_acc=0.9583\n",
      "27 train_acc=0.9655666666666667 / test_acc=0.9597\n",
      "28 train_acc=0.96695 / test_acc=0.9619\n",
      "29 train_acc=0.967 / test_acc=0.9616\n",
      "30 train_acc=0.9650666666666666 / test_acc=0.9617\n",
      "31 train_acc=0.9671 / test_acc=0.9618\n",
      "32 train_acc=0.9663333333333334 / test_acc=0.9611\n",
      "33 train_acc=0.9670166666666666 / test_acc=0.9651\n",
      "34 train_acc=0.9667 / test_acc=0.9626\n",
      "35 train_acc=0.9684166666666667 / test_acc=0.9633\n",
      "36 train_acc=0.9678666666666667 / test_acc=0.9621\n",
      "37 train_acc=0.9669 / test_acc=0.9614\n",
      "38 train_acc=0.96735 / test_acc=0.9631\n",
      "39 train_acc=0.96755 / test_acc=0.9633\n",
      "40 train_acc=0.9682666666666667 / test_acc=0.9631\n",
      "41 train_acc=0.9696166666666667 / test_acc=0.9635\n",
      "42 train_acc=0.96735 / test_acc=0.9613\n",
      "43 train_acc=0.9710833333333333 / test_acc=0.9631\n",
      "44 train_acc=0.9692166666666666 / test_acc=0.9626\n",
      "45 train_acc=0.9684166666666667 / test_acc=0.9622\n",
      "46 train_acc=0.9664666666666667 / test_acc=0.9592\n",
      "47 train_acc=0.96785 / test_acc=0.9613\n",
      "48 train_acc=0.9692666666666667 / test_acc=0.9654\n",
      "49 train_acc=0.9691 / test_acc=0.9631\n",
      "50 train_acc=0.9687333333333333 / test_acc=0.9607\n",
      "51 train_acc=0.9689333333333333 / test_acc=0.9597\n",
      "52 train_acc=0.96925 / test_acc=0.9628\n",
      "53 train_acc=0.9699333333333333 / test_acc=0.964\n",
      "54 train_acc=0.9702166666666666 / test_acc=0.9662\n",
      "55 train_acc=0.9696333333333333 / test_acc=0.9624\n",
      "56 train_acc=0.9716666666666667 / test_acc=0.9661\n",
      "57 train_acc=0.97065 / test_acc=0.9638\n",
      "58 train_acc=0.9714666666666667 / test_acc=0.9653\n",
      "59 train_acc=0.9678666666666667 / test_acc=0.9613\n",
      "60 train_acc=0.9702833333333334 / test_acc=0.9655\n",
      "61 train_acc=0.9712166666666666 / test_acc=0.965\n",
      "62 train_acc=0.96995 / test_acc=0.9665\n",
      "63 train_acc=0.9714666666666667 / test_acc=0.9693\n",
      "64 train_acc=0.9710666666666666 / test_acc=0.9662\n",
      "65 train_acc=0.9721666666666666 / test_acc=0.9659\n",
      "66 train_acc=0.9702333333333333 / test_acc=0.965\n",
      "67 train_acc=0.9716666666666667 / test_acc=0.9671\n",
      "68 train_acc=0.9718833333333333 / test_acc=0.9664\n",
      "69 train_acc=0.9726 / test_acc=0.9682\n",
      "70 train_acc=0.9708 / test_acc=0.9663\n",
      "71 train_acc=0.9723666666666667 / test_acc=0.9672\n",
      "72 train_acc=0.9696 / test_acc=0.9632\n",
      "73 train_acc=0.9712666666666666 / test_acc=0.9663\n",
      "74 train_acc=0.9726 / test_acc=0.9672\n",
      "75 train_acc=0.9705166666666667 / test_acc=0.9658\n",
      "76 train_acc=0.9728 / test_acc=0.9688\n",
      "77 train_acc=0.9732166666666666 / test_acc=0.9679\n",
      "78 train_acc=0.9730333333333333 / test_acc=0.9692\n",
      "79 train_acc=0.97305 / test_acc=0.969\n",
      "80 train_acc=0.9724333333333334 / test_acc=0.9682\n",
      "81 train_acc=0.9725666666666667 / test_acc=0.968\n",
      "82 train_acc=0.97275 / test_acc=0.9681\n",
      "83 train_acc=0.9732666666666666 / test_acc=0.9688\n",
      "84 train_acc=0.9716666666666667 / test_acc=0.964\n",
      "85 train_acc=0.9720833333333333 / test_acc=0.9648\n",
      "86 train_acc=0.9740333333333333 / test_acc=0.9685\n",
      "87 train_acc=0.9744166666666667 / test_acc=0.9681\n",
      "88 train_acc=0.9723666666666667 / test_acc=0.9683\n",
      "89 train_acc=0.9716 / test_acc=0.9685\n",
      "90 train_acc=0.9718166666666667 / test_acc=0.9677\n",
      "91 train_acc=0.9710833333333333 / test_acc=0.9654\n",
      "92 train_acc=0.9709 / test_acc=0.9643\n",
      "93 train_acc=0.9713666666666667 / test_acc=0.9636\n",
      "94 train_acc=0.9736333333333334 / test_acc=0.9673\n",
      "95 train_acc=0.9734833333333334 / test_acc=0.9683\n",
      "96 train_acc=0.9752666666666666 / test_acc=0.9683\n",
      "97 train_acc=0.9734666666666667 / test_acc=0.9666\n",
      "98 train_acc=0.9743333333333334 / test_acc=0.9691\n",
      "99 train_acc=0.97505 / test_acc=0.9698\n",
      "100 train_acc=0.97525 / test_acc=0.9675\n",
      "loss = 5.727633275768255\n",
      "1: train_acc=0.97525 / test_acc=0.9675\n",
      "101.116\n"
     ]
    }
   ],
   "source": [
    "for epoch = 1:2 #iters_num\n",
    "    loss = 0\n",
    "    start_time = now()\n",
    "    iter = 0\n",
    "    for (x_batch, t_batch) in dataloader(x_train, y_train, batch_size=600, shuffle=true)\n",
    "        iter += 1\n",
    "        grads = gradient(network, x_batch, t_batch)\n",
    "        Optimizer.step(optimizer, grads)\n",
    "        loss += criterion(network, x_batch, t_batch)\n",
    "        push!(train_loss_list, loss)\n",
    "        train_acc = accuracy(network, x_train, y_train)\n",
    "        test_acc = accuracy(network, x_test, y_test)\n",
    "        println(\"$(iter) train_acc=$(train_acc) / test_acc=$(test_acc)\")\n",
    "    end\n",
    "\n",
    "    train_acc = accuracy(network, x_train, y_train)\n",
    "    test_acc = accuracy(network, x_test, y_test)\n",
    "    push!(train_acc_list, train_acc)\n",
    "    push!(test_acc_list, test_acc)\n",
    "    @show loss\n",
    "    println(\"$(epoch-1): train_acc=$(train_acc) / test_acc=$(test_acc)\")\n",
    "    println((now() - start_time).value / 1000)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.0",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
